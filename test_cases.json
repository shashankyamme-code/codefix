{
  "description": "Visible test case for KV-Cached Multi-Head Attention",
  "note": "This test case does NOT include expected outputs. Use it to debug your implementation.",
  "test_case": {
    "id": 1,
    "name": "Basic attention without cache",
    "description": "Simple 2-head attention on small sequence - tests fundamental attention computation",
    "config": {
      "d_model": 16,
      "num_heads": 2,
      "max_cache_len": 128,
      "dropout": 0.0
    },
    "seed": 42,
    "inputs": {
      "query": {
        "shape": [1, 4, 16],
        "values": [
          [
            [-0.1468, 0.4395, 0.2620, -0.7663, -0.1386, 1.5211, 0.5612, -0.9093,
              1.3368, -0.7697, 0.5072, -0.5515, -0.3693, 0.5876, -0.5074, 1.1501],
            [0.3283, -1.3581, 0.2369, -0.2802, -0.2469, -0.8446, 0.0168, -0.3449,
              -0.3987, 1.7321, -0.7218, -1.4571, 0.3779, 0.6764, -0.9066, -1.5614],
            [0.4674, 0.0109, -0.2885, 0.9491, -0.5540, -0.2935, -1.2614, -0.6038,
              1.1323, -0.9599, -0.3932, -0.0421, 1.3726, 0.4565, -2.0482, -1.3652],
            [-0.7020, -1.4419, -0.3616, -0.7125, 1.0464, -0.0829, 0.7820, -0.7975,
              0.2172, 0.1832, -1.1805, -1.6946, 0.7881, -0.6467, -0.4535, 0.1587]
          ]
        ]
      },
      "key": {
        "shape": [1, 4, 16],
        "values": [
          [
            [-0.1468, 0.4395, 0.2620, -0.7663, -0.1386, 1.5211, 0.5612, -0.9093,
              1.3368, -0.7697, 0.5072, -0.5515, -0.3693, 0.5876, -0.5074, 1.1501],
            [0.3283, -1.3581, 0.2369, -0.2802, -0.2469, -0.8446, 0.0168, -0.3449,
              -0.3987, 1.7321, -0.7218, -1.4571, 0.3779, 0.6764, -0.9066, -1.5614],
            [0.4674, 0.0109, -0.2885, 0.9491, -0.5540, -0.2935, -1.2614, -0.6038,
              1.1323, -0.9599, -0.3932, -0.0421, 1.3726, 0.4565, -2.0482, -1.3652],
            [-0.7020, -1.4419, -0.3616, -0.7125, 1.0464, -0.0829, 0.7820, -0.7975,
              0.2172, 0.1832, -1.1805, -1.6946, 0.7881, -0.6467, -0.4535, 0.1587]
          ]
        ]
      },
      "value": {
        "shape": [1, 4, 16],
        "values": [
          [
            [0.5697, 0.0369, -0.6685, -0.8235, -0.3440, 0.7100, -0.8458, -0.9679,
              0.0366, -0.2476, -0.2538, -0.6054, 0.4340, -0.2871, -0.3615, -0.9177],
            [1.7007, -0.3473, -0.0693, -0.3835, 0.2561, 0.0811, -1.2119, 0.3246,
              0.2888, 0.7508, -0.7417, -0.1831, 1.0024, -2.0025, 1.0656, 0.2029],
            [-0.5699, -1.0499, 0.8987, -0.3747, -1.2345, 1.1318, -1.5867, 1.6891,
              -0.0575, 0.1985, -0.2552, 1.3032, -0.7808, 0.9979, 0.5299, 0.3439],
            [0.1208, -2.4116, -0.2270, -1.2556, -0.0114, 0.5564, 0.2262, -0.5773,
              1.6795, -1.6672, -0.9298, 0.6530, 0.3607, -0.2969, -1.3108, -0.1172]
          ]
        ]
      },
      "cache": null,
      "use_causal_mask": true
    },
    "expected": null,
    "hint": "This test checks basic attention computation without cache. Debug by:\n1. Verifying output shape is [1, 4, 16]\n2. Checking attention weights sum to 1.0\n3. Ensuring causal mask prevents future positions\n4. Validating scaling factor in attention scores"
  }
}
